import sys
sys.path.insert(0, srcdir('../lcdb-wf'))
import os
from textwrap import dedent
import yaml
import tempfile
import pandas as pd
import numpy as np
import pybedtools
from lcdblib.snakemake import helpers, aligners
from lcdblib.utils import utils
from lib import common, chipseq
from lib.patterns_targets import RNASeqConfig

configfile: 'config/config.yaml'
include: '../lcdb-wf/workflows/references/Snakefile'

shell.prefix('set -euo pipefail; export TMPDIR={};'.format(common.tempdir_for_biowulf()))
shell.executable('/bin/bash')

c = RNASeqConfig(config, 'config/damid_patterns.yaml')


def wrapper_for(path):
    return 'file:' + os.path.join('../lcdb-wf', 'wrappers', 'wrappers', path)

# ----------------------------------------------------------------------------
# RULES
# ----------------------------------------------------------------------------
final_targets = utils.flatten((
    c.targets['bam'],
    utils.flatten(c.targets['fastqc']),
    utils.flatten(c.targets['libsizes']),
    [c.targets['fastq_screen']],
    [c.targets['libsizes_table']],
    [c.targets['multiqc']],
    utils.flatten(c.targets['markduplicates']),
    utils.flatten(c.targets['bigwig']),
))

rule targets:
    """
    Final targets to create
    """
    input: final_targets

# Convert the sampletable to be indexed by the first column, for
# convenience in generating the input/output filenames.
_st = c.sampletable.set_index(c.sampletable.columns[0])

rule combine_files:
    """
    Symlinks files over from original filename
    """
    output: [c.patterns['fastq'].format(sample=sample, sample_dir=c.sample_dir) for sample in _st.index]
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from shutil import copyfileobj
        for record in _st.to_records():
            sample = record[0]
            fnames = record['orig_filenames'].split(',')
            oname = c.patterns['fastq'].format(sample=sample, sample_dir=c.sample_dir)
            with open(oname, 'wb') as wfh:
                for f in fnames:
                    with open(f, 'rb') as rfh:
                        copyfileobj(rfh, wfh)

rule combine_targets:
    input: c.targets['fastq']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1

if 'Run' in c.sampletable.columns and sum(c.sampletable['Run'].str.startswith('SRR')) > 1:

    # Convert the sampletable to be indexed by the first column, for
    # convenience in generating the input/output filenames.
    _st = c.sampletable.set_index(c.sampletable.columns[0])

    rule fastq_dump:
        output:
            c.patterns['fastq']
        resources:
            mem_gb = lambda wildcards, attempt: attempt * 1,
            time_hr = lambda wildcards, attempt: attempt * 1
        run:
            srr = _st.loc[wildcards.sample, 'Run']
            shell(
                'fastq-dump '
                '{srr} '
                '-Z '
                '| gzip -c > {output}.tmp '
                '&& mv {output}.tmp {output} '
            )


rule cutadapt:
    """
    Run cutadapt
    """
    input:
        fastq=c.patterns['fastq']
    output:
        fastq=c.patterns['cutadapt']
    log:
        c.patterns['cutadapt'] + '.log'
    params:
        extra='-a file:../lcdb-wf/include/adapters.fa -q 20 --minimum-length=25'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    wrapper:
        wrapper_for('cutadapt')


rule fastqc:
    """
    Run FastQC
    """
    input: '{sample_dir}/{sample}/{sample}{suffix}'
    output:
        html='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.html',
        zip='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.zip',
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    wrapper:
        wrapper_for('fastqc')


rule bowtie2:
    """
    Map reads with Bowtie2
    """
    input:
        fastq=rules.cutadapt.output.fastq,
        index=[c.refdict[c.assembly][config['aligner']['tag']]['bowtie2']]
    output:
        bam=c.patterns['bam']
    log:
        c.patterns['bam'] + '.log'
    params:
        samtools_view_extra="--threads 4 -q 20",
        samtools_sort_extra='--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 4
    threads: 6
    wrapper:
        wrapper_for('bowtie2/align')


rule fastq_count:
    """
    Count reads in a FASTQ file
    """
    input:
        fastq='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz'
    output:
        count='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz.libsize'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'zcat {input} | echo $((`wc -l`/4)) > {output}'


rule bam_count:
    """
    Count reads in a BAM file
    """
    input:
        bam='{sample_dir}/{sample}/{sample}{suffix}.bam'
    output:
        count='{sample_dir}/{sample}/{sample}{suffix}.bam.libsize'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'samtools view -c {input} > {output}'


rule bam_index:
    """
    Index a BAM
    """
    input:
        bam='{prefix}.bam'
    output:
        bai='{prefix}.bam.bai'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'samtools index {input} {output}'


rule fastq_screen:
    """
    Run fastq_screen to look for contamination from other genomes
    """
    input:
        fastq=rules.cutadapt.output.fastq,
        dm6=c.refdict['dmel'][config['aligner']['tag']]['bowtie2'],
        rRNA=c.refdict[c.assembly][config['rrna']['tag']]['bowtie2'],
        phix=c.refdict['phix']['default']['bowtie2'],
        ercc=c.refdict['ercc']['srm2374']['bowtie2'],
        hg19=c.refdict['human']['gencode-v19']['bowtie2'],
        wolbachia=c.refdict['wolbachia']['default']['bowtie2'],
        ecoli=c.refdict['ecoli']['default']['bowtie2'],
        yeast=c.refdict['sacCer3']['default']['bowtie2'],
    output:
        txt=c.patterns['fastq_screen']
    log:
        c.patterns['fastq_screen'] + '.log'
    params: subset=100000
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 2
    wrapper:
        wrapper_for('fastq_screen')


rule libsizes_table:
    """
    Aggregate fastq and bam counts in to a single table
    """
    input:
        utils.flatten(c.targets['libsizes'])
    output:
        json=c.patterns['libsizes_yaml'],
        tsv=c.patterns['libsizes_table']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    run:
        def sample(f):
            return os.path.basename(os.path.dirname(f))

        def million(f):
            return float(open(f).read()) / 1e6

        def stage(f):
            return os.path.basename(f).split('.', 1)[1].replace('.gz', '').replace('.count', '')

        df = pd.DataFrame(dict(filename=list(map(str, input))))
        df['sample'] = df.filename.apply(sample)
        df['million'] = df.filename.apply(million)
        df['stage'] = df.filename.apply(stage)
        df = df.set_index('filename')
        df = df.pivot('sample', columns='stage', values='million')

        # make nicer column names
        convert = {
            'fastq.libsize': 'stage1_raw',
            'cutadapt.fastq.libsize' : 'stage2_trimmed',
            'cutadapt.bam.libsize': 'stage3_aligned',
            'cutadapt.nodups.bam.libsize': 'stage5_nodups',
        }

        df.columns = [convert[i] for i in df.columns]

        df.to_csv(output.tsv, sep='\t')
        y = {
            'id': 'libsizes_table',
            'section_name': 'Library sizes',
            'description': 'Library sizes at various stages of the pipeline',
            'plot_type': 'table',
            'pconfig': {
                'id': 'libsizes_table_table',
                'title': 'Library size table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json()),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)


rule multiqc:
    """
    Aggregate various QC stats and logs into a single HTML report with MultiQC
    """
    input:
        files=(
            utils.flatten(c.targets['fastqc']) +
            utils.flatten(c.targets['libsizes_yaml']) +
            utils.flatten(c.targets['cutadapt']) +
            utils.flatten(c.targets['bam']) +
            utils.flatten(c.targets['markduplicates']) +
            utils.flatten(c.targets['fastq_screen'])
        ),
        config='config/multiqc_config.yaml'
    output: c.targets['multiqc']
    params:
        analysis_directory=" ".join([c.sample_dir, c.agg_dir]),
        extra='--config config/multiqc_config.yaml',
        outdir=os.path.dirname(c.targets['multiqc'][0]),
        basename=os.path.basename(c.targets['multiqc'][0])
    log: c.targets['multiqc'][0] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'LC_ALL=en_US.UTF.8 LC_LANG=en_US.UTF-8 '
        'multiqc '
        '--quiet '
        '--outdir {params.outdir} '
        '--force '
        '--filename {params.basename} '
        '--config config/multiqc_config.yaml '
        '{params.analysis_directory} '
        '&> {log} '


rule markduplicates:
    """
    Mark or remove PCR duplicates with Picard MarkDuplicates
    """
    input:
        bam=c.patterns['bam']
    output:
        bam=c.patterns['markduplicates']['bam'],
        metrics=c.patterns['markduplicates']['metrics']
    log:
        c.patterns['markduplicates']['bam'] + '.log'
    params:
        java_args='-Xmx12g'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 4
    shell:
        'picard '
        '{params.java_args} '
        'MarkDuplicates '
        'INPUT={input.bam} '
        'OUTPUT={output.bam} '
        'REMOVE_DUPLICATES=true '
        'METRICS_FILE={output.metrics} '
        '&> {log}'


rule bigwig_neg:
    """
    Create a bigwig for negative-strand reads
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        bai=c.patterns['markduplicates']['bam'] + '.bai',
    output: c.patterns['bigwig']['neg']
    threads: 8
    log:
        c.patterns['bigwig']['neg'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--filterRNAstrand forward '
        '--normalizeUsing RPKM '
        '--extendReads 300 '
        '&> {log}'


rule bigwig_pos:
    """
    Create a bigwig for postive-strand reads.
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        bai=c.patterns['markduplicates']['bam'] + '.bai',
    output: c.patterns['bigwig']['pos']
    threads: 8
    log:
        c.patterns['bigwig']['pos'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--filterRNAstrand reverse '
        '--normalizeUsing RPKM '
        '--extendReads 300 '
        '&> {log}'


# vim: ft=snakemake.python
